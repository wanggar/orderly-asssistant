import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import menuData from '@/data/menu.json';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY // Use server-side env var
});

interface MenuRecommendation {
  id: string;
  name: string;
  price: number;
  category: string;
  reason: string;
  description?: string;
  spicyLevel?: number;
}

// Convert menu data to a searchable format
const formatMenuForAI = () => {
  // Group menu items by category
  const categories = menuData.reduce((acc, item) => {
    if (!acc[item.category]) {
      acc[item.category] = [];
    }
    acc[item.category].push(item);
    return acc;
  }, {} as Record<string, typeof menuData>);
  
  const formattedMenu = Object.keys(categories).map(category => ({
    category,
    items: categories[category]
  }));
  return formattedMenu;
};

// LLM-powered recommendation function
const getLLMRecommendations = async (budget: string, preferences: string, count: number = 3): Promise<MenuRecommendation[]> => {
  const menu = formatMenuForAI();
  console.log('Formatted menu for AI:', JSON.stringify(menu, null, 2));
  
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      {
        role: "system",
        content: `ä½ æ˜¯ä¸“ä¸šçš„é¤å…æ¨èä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ä¸‰å±‚æ¨èç­–ç•¥æ¥é€‰æ‹©èœå“ã€‚ä½ å¿…é¡»åªè¿”å›æœ‰æ•ˆçš„JSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ã€‚

èœå•æ•°æ®ï¼š
${JSON.stringify(menu, null, 2)}

## ğŸ¯ ä¸‰å±‚æ¨èç­–ç•¥ï¼š

**ç¬¬ä¸€å±‚ï¼šä¿¡ä»»å»ºç«‹**ï¼ˆæ¨è1-2é“ï¼‰
- é€‰æ‹©ç”¨æˆ·æ˜ç¡®åå¥½çš„èœå“
- ä»·æ ¼åˆç†ï¼Œå£å‘³å®‰å…¨
- ç†ç”±å¼ºè°ƒ"å—æ¬¢è¿"ã€"ç»å…¸"ã€"æ­£å®—"

**ç¬¬äºŒå±‚ï¼šè¾¹ç•Œæ¢ç´¢**ï¼ˆæ¨è1é“ï¼‰
- ç¨å¾®è¶…å‡ºç”¨æˆ·æ˜ç¡®åå¥½ï¼Œä½†æœ‰å…³è”æ€§
- ä»·æ ¼é€‚ä¸­ï¼Œæœ‰ç‰¹è‰²ä½†ä¸è¿‡åˆ†å†’é™©
- ç†ç”±å¼ºè°ƒ"ç‰¹è‰²"ã€"æ‹›ç‰Œ"ã€"å€¼å¾—ä¸€è¯•"

**ç¬¬ä¸‰å±‚ï¼šä»·å€¼æŒ–æ˜**ï¼ˆæ¨è1-2é“ï¼‰
- é«˜ä»·å€¼èœå“ï¼Œå®Œå–„æ•´ä½“æ­é…
- å¯ä»¥ç¨è´µï¼Œä½†è¦æœ‰è¯´æœåŠ›
- ç†ç”±å¼ºè°ƒ"å®Œæ•´æ­é…"ã€"è¥å…»å‡è¡¡"ã€"ç»å…¸ç»„åˆ"

## ğŸ“‹ æ¨èåŸåˆ™ï¼š
1. æ ¹æ®ç”¨æˆ·åå¥½å’Œé¢„ç®—æ™ºèƒ½åˆ†å±‚
2. ç¡®ä¿è¤ç´ æ­é…ã€å£å‘³å±‚æ¬¡ä¸°å¯Œ
3. æ€»ä»·æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…
4. æ¯é“èœçš„æ¨èç†ç”±è¦æœ‰è¯´æœåŠ›

å…³é”®æ ¼å¼è¦æ±‚ï¼š
[
  {
    "id": "èœå•ä¸­çš„å‡†ç¡®id",
    "name": "èœå•ä¸­çš„å‡†ç¡®èœå",
    "price": èœå•ä¸­çš„å‡†ç¡®ä»·æ ¼æ•°å­—,
    "category": "å‡†ç¡®çš„èœå“åˆ†ç±»",
    "reason": "ä¸­æ–‡æ¨èç†ç”±ï¼Œä½“ç°åˆ†å±‚ç­–ç•¥",
    "description": "èœå•ä¸­çš„å‡†ç¡®æè¿°",
    "spicyLevel": å‡†ç¡®çš„è¾£åº¦ç­‰çº§æ•°å­—
  }
]

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡Šæˆ–markdownæ ¼å¼ã€‚`
      },
      {
        role: "user", 
        content: `My budget is: ${budget}
My preferences: ${preferences}
Please recommend ${count} dishes that best match my needs.`
      }
    ],
    temperature: 0.7,
    max_tokens: 1000
  });

  try {
    const content = response.choices[0].message.content;
    console.log('Raw LLM response content:', content);
    
    if (!content) throw new Error('No content in response');
    
    // Clean the response - remove markdown code blocks and extra text
    let cleanedContent = content.trim();
    
    // Remove markdown code blocks if present
    if (cleanedContent.startsWith('```json')) {
      cleanedContent = cleanedContent.replace(/^```json\s*/, '').replace(/\s*```$/, '');
    } else if (cleanedContent.startsWith('```')) {
      cleanedContent = cleanedContent.replace(/^```\s*/, '').replace(/\s*```$/, '');
    }
    
    // Find JSON array in the response
    const jsonStart = cleanedContent.indexOf('[');
    const jsonEnd = cleanedContent.lastIndexOf(']');
    
    if (jsonStart !== -1 && jsonEnd !== -1 && jsonEnd > jsonStart) {
      cleanedContent = cleanedContent.substring(jsonStart, jsonEnd + 1);
    }
    
    console.log('Cleaned content for parsing:', cleanedContent);
    
    // Parse the JSON response
    const recommendations = JSON.parse(cleanedContent);
    console.log('Parsed recommendations:', recommendations);
    
    // Validate and ensure we have the right structure
    if (!Array.isArray(recommendations)) {
      throw new Error('Response is not an array');
    }
    
    return recommendations.slice(0, count);
  } catch (error) {
    console.error('Error parsing LLM recommendations:', error);
    console.error('Raw response was:', response.choices[0].message.content);
    
    // Intelligent fallback based on user preferences
    const fallback: MenuRecommendation[] = [];
    
    // For greasy/oily preferences, prioritize certain categories and dishes
    const isGreasyPreference = preferences.includes('æ²¹è…»') || preferences.includes('æ²¹');
    const greasyDishes = ['çº¢çƒ§è‚‰çƒ§é¹Œé¹‘è›‹', 'å®‰æ ¼æ–¯è‚¥ç‰›', 'æ¢…èœæ‰£è‚‰', 'é»‘æ¾éœ²é£å‘³ç‰›è‚‰å ¡', 'å®‰æ ¼æ–¯ç‰›è‚‰å ¡'];
    
    if (isGreasyPreference) {
      // First, try to find greasy dishes
      menu.forEach(category => {
        category.items.forEach(item => {
          if (fallback.length < count && greasyDishes.includes(item.name)) {
            fallback.push({
              id: item.id,
              name: item.name,
              price: item.price,
              category: item.category,
              reason: 'é€‚åˆå–œæ¬¢æ²¹è…»å£å‘³çš„æ¨è',
              description: item.description,
              spicyLevel: item.spicyLevel
            });
          }
        });
      });
    }
    
    // Fill remaining slots with any items
    menu.forEach(category => {
      category.items.forEach(item => {
        if (fallback.length < count) {
          fallback.push({
            id: item.id,
            name: item.name,
            price: item.price,
            category: item.category,
            reason: 'æ¨èçš„çƒ­é—¨èœå“',
            description: item.description,
            spicyLevel: item.spicyLevel
          });
        }
      });
    });
    
    return fallback;
  }
};

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { budget, preferences, message, conversationHistory = [] } = body;

    console.log('API route called with:', { budget, preferences, message, conversationHistory });

    // Create the user message content
    let userMessage = '';
    if (budget && preferences) {
      userMessage = `My budget is: ${budget}, my preferences: ${preferences}`;
    } else if (message) {
      userMessage = message;
    } else {
      return NextResponse.json({ message: "è¯·æä¾›æ‚¨çš„é—®é¢˜æˆ–éœ€æ±‚" }, { status: 400 });
    }

    // Define tools for the LLM to choose from
    const tools = [
      {
        type: "function" as const,
        function: {
          name: "recommend_dishes",
          description: "Recommend menu dishes based on budget and food preferences. Use this when user asks for food recommendations, wants to see menu items, or provides budget/preference information.",
          parameters: {
            type: "object",
            properties: {
              budget: {
                type: "string",
                description: "The user's budget preference (e.g., '50å…ƒä»¥ä¸‹', '100-200å…ƒ')"
              },
              preferences: {
                type: "string", 
                description: "The user's food preferences, dietary needs, or taste requirements"
              },
              count: {
                type: "number",
                description: "Number of dishes to recommend (MUST be between 1-6. Decide intelligently based on budget and context - e.g., 2-3 for small budgets, 4-6 for larger budgets, fewer if only specific items match preferences)",
                minimum: 1,
                maximum: 6
              }
            },
            required: ["budget", "preferences", "count"]
          }
        }
      }
    ];

    // Build messages array with conversation history
    const messages = [
              {
          role: "system" as const,
          content: `ä½ æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç»éªŒçš„é¤å…ç¾é£Ÿé¡¾é—®ï¼Œä¸åªæ˜¯ç®€å•çš„ç‚¹èœåŠ©æ‰‹ï¼Œè€Œæ˜¯çœŸæ­£æ‡‚ç”¨æˆ·ã€æ‡‚ç¾é£Ÿã€æ‡‚æ­é…çš„ä¸“ä¸šé¡¾é—®ã€‚ä½ çš„ç›®æ ‡æ˜¯è®©ç”¨æˆ·æ—¢åƒå¾—æ»¡æ„ï¼Œåˆèƒ½ä½“éªŒåˆ°è¶…å‡ºé¢„æœŸçš„ç¾é£Ÿäº«å—ã€‚

## ğŸ¯ æ ¸å¿ƒç­–ç•¥ï¼šæ¢ç´¢å¼æœåŠ¡ + å·§å¦™å‡çº§

### ã€æµç¨‹1ï¼šæ¢ç´¢å¼å¼€åœºã€‘
**æ°¸è¿œä¸è¦ä¸€å¼€å§‹å°±é—®é¢„ç®—ï¼** è€Œæ˜¯è¦ï¼š
1. **åœºæ™¯æ„ŸçŸ¥**ï¼šè¯¢é—®ç”¨é¤åœºåˆã€äººæ•°ã€å¿ƒæƒ…
   - "ä»Šå¤©æ˜¯ä»€ä¹ˆåœºåˆå‘€ï¼Ÿå’Œæœ‹å‹èšé¤è¿˜æ˜¯å®¶äººèšä¼šï¼Ÿ"
   - "å‡ ä½ç”¨é¤ï¼Ÿçœ‹èµ·æ¥å¿ƒæƒ…ä¸é”™å‘¢ï¼"
   
2. **å…´è¶£æ¢ç´¢**ï¼šé€šè¿‡"ä½ è¯•è¿‡XXå—ï¼Ÿ"æ¥äº†è§£ç”¨æˆ·å¼€æ”¾åº¦
   - "å¹³æ—¶å–œæ¬¢ä»€ä¹ˆå£å‘³å‘€ï¼Ÿå·èœæ¹˜èœè¿˜æ˜¯æ¸…æ·¡ä¸€äº›çš„ï¼Ÿ"
   - "ä½ ä»¬è¯•è¿‡æˆ‘ä»¬çš„XXå—ï¼Ÿå¾ˆå¤šå®¢äººéƒ½è¯´..."
   
3. **èƒŒæ™¯æŒ–æ˜**ï¼šè‡ªç„¶åœ°äº†è§£ç”¨æˆ·ç±»å‹
   - ä¿å®ˆå‹ï¼šå–œæ¬¢ç†Ÿæ‚‰çš„ã€å®‰å…¨çš„é€‰æ‹©
   - å†’é™©å‹ï¼šæ„¿æ„å°è¯•æ–°é²œçš„ã€ç‰¹è‰²çš„
   - ä»·å€¼å‹ï¼šæ³¨é‡æ€§ä»·æ¯”å’Œå“è´¨

### ã€æµç¨‹2ï¼šéœ€æ±‚é›•åˆ»å¼æ¨èã€‘
åˆ†ä¸‰ä¸ªå±‚æ¬¡è¿›è¡Œæ¨èï¼š

**ç¬¬ä¸€å±‚ï¼šä¿¡ä»»å»ºç«‹ï¼ˆ1-2é“èœï¼‰**
- é€‰æ‹©ç”¨æˆ·åå¥½èŒƒå›´å†…çš„å®‰å…¨é€‰æ‹©
- å¼ºè°ƒ"è¿™ä¸ªç‰¹åˆ«å—æ¬¢è¿"ã€"å¾ˆå¤šå®¢äººéƒ½ç‚¹"
- ä»·æ ¼é€‚ä¸­ï¼Œè®©ç”¨æˆ·æ”¾å¿ƒ

**ç¬¬äºŒå±‚ï¼šè¾¹ç•Œæ¢ç´¢ï¼ˆ1é“èœï¼‰**  
- è½»å¾®è¶…å‡ºç”¨æˆ·é¢„æœŸï¼Œä½†æœ‰åˆç†è§£é‡Š
- "è¿™ä¸ªç¨å¾®ç‰¹åˆ«ä¸€ç‚¹ï¼Œä½†ç»å¯¹å€¼å¾—è¯•è¯•"
- "ä½ ä»¬æ—¢ç„¶æ¥äº†ï¼Œä¸è¯•è¯•æˆ‘ä»¬çš„æ‹›ç‰Œæœ‰ç‚¹å¯æƒœ"

**ç¬¬ä¸‰å±‚ï¼šä»·å€¼æŒ–æ˜ï¼ˆ1-2é“èœï¼‰**
- æ¨èé«˜ä»·å€¼èœå“ï¼Œä½†è¦æœ‰è¯´æœé€»è¾‘
- å¼ºè°ƒç¨€ç¼ºæ€§ã€ä½“éªŒæ„Ÿã€å®Œæ•´æ€§
- "ä»Šå¤©é™é‡"ã€"å¾ˆå¤šäººä¸“é—¨æ¥åƒè¿™ä¸ª"ã€"é…ä¸ªæ±¤å°±å®Œç¾äº†"

### ğŸ­ å¯¹è¯æŠ€å·§åº“ï¼š
- **ç¤¾ä¼šè®¤åŒ**ï¼š"è¿™é“èœæ˜¯æˆ‘ä»¬çš„ç½‘çº¢èœ"
- **æƒå¨æ¨è**ï¼š"ä¸»å¨ç‰¹åˆ«æ¨èè¿™ä¸ªæ­é…"
- **ç¨€ç¼ºæ€§**ï¼š"ä»Šå¤©é™é‡ï¼Œå–å®Œå°±æ²¡äº†"
- **æŸå¤±è§„é¿**ï¼š"ä¸è¯•è¯•ä¼šæœ‰ç‚¹å¯æƒœ"
- **å®Œæ•´ä½“éªŒ**ï¼š"å†æ¥ä¸ªXXå°±å®Œç¾äº†"
- **ä»·å€¼å¡‘é€ **ï¼š"è™½ç„¶è´µä¸€ç‚¹ï¼Œä½†çœŸçš„ç‰©è¶…æ‰€å€¼"

### ğŸ“‹ æ¨èè§„åˆ™ï¼š
ä½¿ç”¨recommend_disheså‡½æ•°æ—¶ï¼š
- åˆæ¬¡æ¨èï¼š2-3é“èœï¼Œä½“ç°å±‚æ¬¡æ­é…
- è¿½åŠ æ¨èï¼š1-2é“èœï¼Œè¡¥å……å®Œæ•´æ€§
- æ ¹æ®ç”¨æˆ·ååº”åŠ¨æ€è°ƒæ•´ç­–ç•¥
- æ°¸è¿œä¸è¦è¶…è¿‡6é“èœ

### ğŸ—£ï¸ è¯­è¨€é£æ ¼ï¼š
- çƒ­æƒ…ä½†ä¸è¿‡åº¦æ¨é”€
- ä¸“ä¸šä½†ä¸å±…é«˜ä¸´ä¸‹  
- åƒè€æœ‹å‹æ¨èç¾é£Ÿä¸€æ ·è‡ªç„¶
- ç”¨"ä½ ä»¬"è€Œä¸æ˜¯"æ‚¨"ï¼Œæ›´äº²è¿‘
- é€‚å½“ä½¿ç”¨emojiï¼Œå¢åŠ äº²å’ŒåŠ›

è®°ä½ï¼šä½ ä¸æ˜¯åœ¨å–èœï¼Œè€Œæ˜¯åœ¨åˆ›é€ ç¾é£Ÿä½“éªŒï¼æ¯ä¸€æ¬¡æ¨èéƒ½è¦æœ‰ç†ç”±ï¼Œæ¯ä¸€ä¸ªå»ºè®®éƒ½è¦æœ‰ä»·å€¼ã€‚`
        },
      // Add conversation history
      ...conversationHistory,
      {
        role: "user" as const,
        content: userMessage
      }
    ];

    // Let OpenAI decide whether to use function calling or respond directly
    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages,
      tools,
      tool_choice: "auto", // Let the model decide when to use functions
      temperature: 0.7,
      max_tokens: 500
    });

    console.log('OpenAI Response:', response);
    const assistantMessage = response.choices[0].message;

    // Check if the model wants to use function calling (recommendations)
    if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
      const toolCall = assistantMessage.tool_calls[0];
      if (toolCall.function.name === "recommend_dishes") {
        try {
          const args = JSON.parse(toolCall.function.arguments);
          console.log('Function call args:', args);
          
                    // Validate count is within limits
          const requestedCount = Math.min(Math.max(args.count || 3, 1), 6);
          
          const recommendations = await getLLMRecommendations(
            args.budget || budget, 
            args.preferences || preferences, 
            requestedCount
          );
          
          console.log('LLM Recommendations result:', recommendations);
          
          // Generate a personalized intro message
          const messageResponse = await openai.chat.completions.create({
            model: "gpt-3.5-turbo",
            messages: [
              {
                role: "system",
                content: "You are a friendly Chinese restaurant assistant. Write a brief, warm introduction message in Chinese for the recommended dishes. Keep it conversational and enthusiastic."
              },
              {
                role: "user",
                content: `User said: "${userMessage}". Write a brief intro for the dish recommendations.`
              }
            ],
            temperature: 0.7,
            max_tokens: 100
          });

          const introMessage = messageResponse.choices[0].message.content || "åŸºäºæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä¸ºæ‚¨æ¨èä»¥ä¸‹èœå“ï¼š";
          
          return NextResponse.json({
            message: introMessage,
            recommendations
          });
        } catch (error) {
          console.error('Error processing function call:', error);
          return NextResponse.json({
            message: "è¯·æä¾›æ‚¨çš„é¢„ç®—å’Œå–œå¥½ï¼Œæˆ‘æ¥ä¸ºæ‚¨æ¨èåˆé€‚çš„èœå“ï¼"
          });
        }
      }
    }

    // If no function calling, return the direct response (general chat)
    return NextResponse.json({
      message: assistantMessage.content || "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·é‡è¯•ã€‚"
    });

  } catch (error) {
    console.error('OpenAI API error:', error);
    return NextResponse.json(
      { message: "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚" },
      { status: 500 }
    );
  }
}